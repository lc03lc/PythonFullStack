scrapy框架
-什么是框架
    -集成了很多功能并且具有很强通用性的一个项目模板
-如何学习框架
    -专门学习框架封装的各种详细用法
-什么是scrapy
    -爬虫中封装好的一个明星框架。功能：高性能的持久化存储的操作，异步的数据下载操作，高性能的数据解析操作，分布式
-scrapy框架的基本使用
    -环境的安装
    -创建一个工程：scrapy startproject xxxPro
    -在spiders子目录中创建一个爬虫文件
        -scrapy genspider spiderName www.xxx.com
    -执行工程：
        -scrapy crawl spiderName
-scrapy数据解析
-scrapy持久化存储
    -基于终端指令：
        -要求：只可以将parse方法的返回值存储到本地的文本文件中
        -注意：只能存储为'json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle'文件格式
        -指令： scrapy crawl xxx -o filePath
        -好处：非常简洁高效
        -缺点：局限性（数据值可以存储到指定后缀的格式中）
    -基于管道
        -编码流程
            -数据解析
            -在item类中定义相关属性
            -将解析的数据封装到item类型的对象
            -将item类型的对象提交给管道进行持久化存储
            -在管道的process_item中将其接收到的item对象中存储的数据进行持久化存储操作
            -在配置文件中开启管道
        -好处：通用性强
-基于Spider全站数据的爬取
    -将网站中某板块下的全部页码对应的页面数据进行页面爬取和解析的操作
    -需求：爬取校花网的照片名称
    -实现方式：
        -将所有页面的url添加到start_url中（不推荐）
        -自行手动进行请求发送（推荐）
            -yield scrapy.Request(url,callback=self.parse)
-请求传参
    -使用场景：如果要爬取解析的数据不在同一张页面中。（深度爬取）
    -需求：爬取Boss直聘的岗位名称和岗位描述
-图片数据的爬取之ImagesPipeline
    -基于scrapy爬取字符串类型的数据和爬取图片类型的数据区别？
        -字符串：只需要基于xpath进行解析且提交管道进行持久化存储
        -图片：只可以xpath解析出图片src属性值。单独对图片地址发起请求，获取图片二进制类型的数据
    -ImagesPipeline：
        -只需要对img的src属性值进行解析，提交的管道，管道就会对图片的src进行请求发送，获取图片的二进制类型数据，且还会进行持久化存储
    -需求：爬取站长素材高清图片
    -使用流程：
        -数据解析（图片的src）
        -将存储图片地址的item提交给指定的管道类
        -在管道文件中自己定义一个基于ImagesPipeLine的管道类
            -get_media_request
            -file_path
            -item_completed
        -在配置文件(settings)中
            -指定图片存储的目录：IMAGES_STORE
            -开启自定义的管道类
-中间件
    -下载中间件
        -位置：引擎和下载器之间
        -作用：批量拦截到整个工程在所有的请求和响应
        -拦截请求：
            -UA伪装
            -代理IP
        -拦截响应
            -篡改响应数据、响应对象
            -需求：爬取网易新闻的标题和数据（标题和内容）
                -1、通过网易新闻首页解析出url（可以直接爬取）
                -2、板块内部爬取新闻标题（动态加载）
                -3、通过解析出每一条新闻详情页的url获取详情页的页面源码，解析出新闻的内容
-CrawlSpider类：Spider的一个子类
    -全站数据爬取的方式
        -基于Spider
        -基于CrawlSpider
    -CrawlSpider的使用
        -创建一个工程
        -创建爬虫文件（CrawlSpider）
            -scrapy genspider -t crawl spiderName www.xxx.com
            -链接提取器LinkExtrator:
                -根据指定的规则（allow）进行指定链接的提取
            -规则解析器
                -将链接提取器进行制定规则的解析